# logger options
image_save_iter: 10000         # How often do you want to save output images during training
display_size: 10              # How many images do you want to display each time
snapshot_save_iter: 10000     # How often do you want to save trained models
log_iter: 10                  # How often do you want to log the training stats

# optimization options
max_iter: 100000              # maximum number of training iterations
train_seg_iters: 5000         # number of iters before starting training the segmentation network
guide_gen_iters: 5100         # number of iters before starting guiding the translation
batch_size: 16                # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 25000              # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 1                      # weight of adversarial loss
recon_x_w: 10                 # weight of image reconstruction loss
recon_s_w: 1                  # weight of style reconstruction loss
recon_c_w: 1                  # weight of content reconstruction loss
recon_x_cyc_w: 10             # weight of explicit style augmented cycle consistency loss
sem_w: 10                     # weight of semantic loss

# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 64                 # number of filters in MLP
  style_dim: 8                # length of style code
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 1             # number of downsampling layers in content encoder
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: reflect           # padding type [zero/reflect]
dis:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 3                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/nsgan]
  num_scales: 1               # number of scales
  pad_type: reflect           # padding type [zero/reflect]
seg:
  mode: batch_norm            # adaptation mode [parallel_adapters/batch_norm]
  isproj: [False, False]      # position of the adaptation module (parallel adapters)
  nb_domains: 2               # number of domains
  factor: 2                   # width factor of the network
  build_blocks: [1, 1, 1]     # number of blocks per layer
  dr_val: 0.5                 # dropout value
  dropout: True               # dropout [True/False]
  ignore_index: 3             # target value that is ignored and does not contribute to the input gradient

# data options
input_dim_a: 5                 # number of input channels
input_dim_b: 20                # number of input channels
num_workers: 4                 # number of data loading threads

outputs: ['real', 'recon', 'cycle_recon', 'fake_1', 'fake_2']